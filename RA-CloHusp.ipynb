{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## General Description -\n",
        "\n",
        "- This is the Python implementation of two proposed algorithms -\n",
        "  -  CLOHusp - Closed High Utility Sequential Pattern Mining\n",
        "\n",
        "  - RACLOHusp - Redundancy Aware Closed High Utility Sequential Pattern Mining\n",
        "\n",
        "- Developed and implemented by -\n",
        "  - Md Aminul Kader Bulbul\n",
        "  - Reg. no - 2017814980\n",
        "  - MS Student, CSEDU\n"
      ],
      "metadata": {
        "id": "K3UfDVyZ7rZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "skbEmftiefek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19227ea5-ce03-4161-dd3c-f197531f19a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary libraries"
      ],
      "metadata": {
        "id": "hqEWvocXaZJO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzc1krFSW55-"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "\n",
        "import math\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pickle\n",
        "import time\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declare global variables"
      ],
      "metadata": {
        "id": "SDyumSkEaiO2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR5UcwDXupG2"
      },
      "outputs": [],
      "source": [
        "# Declare global variables\n",
        "\n",
        "df_chus = None\n",
        "\n",
        "item_info = {}\n",
        "\n",
        "unique_items = []\n",
        "seq_util_val = []\n",
        "util_list = []\n",
        "seq_list = []\n",
        "\n",
        "seq_no = {}\n",
        "seq_pos_no = {}\n",
        "seq_pos_no_util = {}\n",
        "seq_pos_no_rem_util = {}\n",
        "seq_pos_no_rbu = {}\n",
        "seq_pos_no_lru = {}\n",
        "seq_pos_no_rem_item_count = {}\n",
        "item_seen_bef = [0] * 1570\n",
        "\n",
        "item_neighbour = {}\n",
        "\n",
        "temp_bitmap = []\n",
        "\n",
        "keys_to_delete = []\n",
        "\n",
        "item_info_dbv = {}\n",
        "\n",
        "pattern_av_hash = {}\n",
        "\n",
        "chus_hash = {}\n",
        "\n",
        "visited = []\n",
        "cluster_info = {}\n",
        "cluster_info_per_reps = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function implementation of Early Elimination by Support and SWU equivalence.\n",
        "\n",
        "\n",
        "*   It checks whether pattern item_l is a sub-pattern or super-pattern of any other pattern in the database.\n",
        "*   If it is a sub-pattern then merge it with super-pattern and stop further exploration.\n",
        "*   Else if it is a super-pattern, then merge corresponding sub-patterns with it and stop further exploration.\n",
        "\n"
      ],
      "metadata": {
        "id": "NhSHCuS_cP2S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcPh8iEaEW8q"
      },
      "outputs": [],
      "source": [
        "def pattern_avoidable(item_l):\n",
        "\n",
        "  \"\"\"\n",
        "    Function implementation of Early Elimination by Support and SWU equivalence.\n",
        "        - It checks whether pattern item_l is a sub-pattern or super-pattern of any other pattern in the database.\n",
        "        - If it is a sub-pattern then merge it with super-pattern and stop further exploration.\n",
        "        - Else if it is a super-pattern, then merge corresponding sub-patterns with it and stop further exploration.\n",
        "  \"\"\"\n",
        "\n",
        "  _key_ = item_info_dbv[tuple(item_l)]['rem_item_count'] # set remaining item count as key of a dictionary\n",
        "  swu_itemset_p = item_info_dbv[tuple(item_l)]['swu'] # Get the SWU value of item_l\n",
        "  sup_itemset_p = item_info_dbv[tuple(item_l)]['support'] # Get the Support value of item_l\n",
        "\n",
        "\n",
        "  if(_key_ not in pattern_av_hash): # If key is new\n",
        "    pattern_av_hash[_key_] = [] # Declare an empty list against it\n",
        "\n",
        "  mk = pattern_av_hash[_key_] # Get the list of patterns at key\n",
        "\n",
        "  if(len(mk)==0): # if key has no pattern\n",
        "    pattern_av_hash[_key_].append(item_l) # add item_l pattern to this key\n",
        "    return False # Explore this pattern\n",
        "\n",
        "  else:\n",
        "    flag = 0\n",
        "    for itemset_m in range(len(mk)): # for each pattern in the key\n",
        "      if(itemset_m >= len(mk)): # if index not valid then break\n",
        "        break\n",
        "      itemset_p_bar = mk[itemset_m] # Get a pattern having same number of remaining itemset\n",
        "      swu_itemset_p_bar = item_info_dbv[tuple(itemset_p_bar)]['swu'] # get its swu value\n",
        "      sup_itemset_p_bar = item_info_dbv[tuple(itemset_p_bar)]['support'] # get its support value\n",
        "\n",
        "      if(swu_itemset_p_bar == swu_itemset_p and\n",
        "         sup_itemset_p_bar == sup_itemset_p): # If these 2 patterns have same SWU and Support value\n",
        "        l_p = len(item_l)\n",
        "        l_p_bar = len(itemset_p_bar)\n",
        "        if(l_p <= l_p_bar and item_l[l_p-1] == itemset_p_bar[l_p_bar-1]): # if {item_l} is BACKWARD-SUB-PATTERN of {itemset_p_bar}\n",
        "          return True # No need for further exploration\n",
        "        else:\n",
        "          if(l_p > l_p_bar and item_l[l_p-1] == itemset_p_bar[l_p_bar-1]): #if {item_l} is BACKWARD-SUPER-PATTERN of {itemset_p_bar}\n",
        "            del pattern_av_hash[_key_][itemset_m] # then delete itemset_p_bar\n",
        "            flag = 1 # mark that we entered into 2nd case\n",
        "\n",
        "    if(flag==1): # if 2nd case\n",
        "      pattern_av_hash[_key_].append(item_l) # add item_l into key\n",
        "      return True\n",
        "\n",
        "  pattern_av_hash[_key_].append(item_l) # add item_l into key\n",
        "  return False # Explore this pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to explore the sibling patterns of item_l_child using DFS approach"
      ],
      "metadata": {
        "id": "kkdS_-nlg19l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw2W-Z5E234D"
      },
      "outputs": [],
      "source": [
        "def exp_siblings(item_l_child, s_i_temp, i_i_temp, minsup, minutil):\n",
        "\n",
        "  \"\"\"\n",
        "    Explore the sibling patterns of item_l_child - DFS approach\n",
        "  \"\"\"\n",
        "  chus_cand_set_sib = []\n",
        "  for item_l_child_i in range(len(item_l_child)): # For each pattern\n",
        "    item_l_child_i_main = item_l_child[item_l_child_i]\n",
        "    chus_cand_set_sib += dfs_pattern_ext(item_l_child_i_main, s_i_temp, i_i_temp[item_l_child_i:], minsup, minutil) # Explore it using DFS approach\n",
        "  return chus_cand_set_sib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function implementation of proposed CLOHusp algorithm\n",
        "*   It explores the pattern item_l using DFS approach\n",
        "*   It performs necessary operations on the dynamic vectors to calculate corresponding support, utility, remaining utility, SWU, RBU, LRU etc.\n",
        "*   It performs both the S-Extension and I-extension\n",
        "*   It uses LRU to identify which items are worthy of extension\n",
        "*   It uses RBU to identify whether the pattern has potential to generate high-utility super patterns, else prune it\n",
        "*   It returns the candidate CHUS patterns"
      ],
      "metadata": {
        "id": "mNGslHVwjk3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7T5-Zyx18od"
      },
      "outputs": [],
      "source": [
        "def dfs_pattern_ext(item_l,s_items,i_items,ms,mu):\n",
        "\n",
        "  \"\"\"\n",
        "    Main function implementation of proposed CLOHusp algorithm\n",
        "    - It explores the pattern item_l using DFS approach\n",
        "    - It performs necessary operations on the dynamic vectors to calculate corresponding support, utility, remaining utility, SWU, RBU, LRU etc.\n",
        "    - It performs both the S-Extension and I-extension\n",
        "    - It uses LRU to identify which items are worthy of extension\n",
        "    - It uses RBU to identify whether the pattern has potential to generate high-utility super patterns, else prune it\n",
        "    - It returns the candidate CHUS patterns\n",
        "  \"\"\"\n",
        "\n",
        "  if(not pattern_avoidable(item_l)): # if pattern item_l needs to be explored\n",
        "\n",
        "    s_i_temp = []\n",
        "    i_i_temp = []\n",
        "    item_l_child = []\n",
        "    chus_cand_set = []\n",
        "\n",
        "    # S - Extension\n",
        "    for sindx in range(len(s_items)): # for each s-items\n",
        "      s_i = s_items[sindx]\n",
        "\n",
        "      if((s_i not in item_neighbour[item_l[-1]]) or (s_i in item_l)): # if s-item is not neighbour to item_l\n",
        "        continue # then no need to extend with it\n",
        "\n",
        "      # Otherwise, explore with it\n",
        "      # Get corresponding parameters\n",
        "      givitem_sup_bmp_s = item_info_dbv[tuple(item_l)]['sup_bmp_s_sep']\n",
        "      givitem_rbu_bmp = item_info_dbv[tuple(item_l)]['rbu_bmp_sep']\n",
        "      sitem_sup_bmp = item_info_dbv[tuple([s_i])]['sup_bmp_sep']\n",
        "\n",
        "      givitem_util_bmp_s = item_info_dbv[tuple(item_l)]['util_bmp_s_sep']\n",
        "      sitem_util_bmp = item_info_dbv[tuple([s_i])]['util_bmp_sep']\n",
        "      sitem_rem_util_bmp = item_info_dbv[tuple([s_i])]['rem_util_bmp_sep']\n",
        "\n",
        "      sitem_swu_bmp = item_info_dbv[tuple([s_i])]['swu_bmp_sep']\n",
        "      sitem_pos_rem_itc_bmp = item_info_dbv[tuple([s_i])]['pos_bmp_sep']\n",
        "\n",
        "      giv_s_item_sup_bmp = []\n",
        "      giv_s_item_lru_bmp = []\n",
        "      giv_s_item_util_bmp = []\n",
        "      giv_s_item_remutil_bmp = []\n",
        "      giv_s_item_rbu_bmp = []\n",
        "      giv_s_item_swu_bmp = []\n",
        "      giv_s_item_pos_rem_itc_bmp = []\n",
        "\n",
        "      giv_s_sup = 0\n",
        "      giv_s_lru = 0\n",
        "      giv_s_util = 0\n",
        "      giv_s_seq_sum = 0\n",
        "      giv_s_remutil = 0\n",
        "      giv_s_swu = 0\n",
        "      giv_s_rbu = 0\n",
        "      giv_s_pos_rem_itc = 0\n",
        "\n",
        "      giv_s_item_sup_bmp_s = []\n",
        "      giv_s_item_util_bmp_s = []\n",
        "\n",
        "      # Calculate corresponding support, utility, remaining utility, SWU, RBU, LRU etc.\n",
        "      for zk in range(len(givitem_sup_bmp_s)):\n",
        "\n",
        "        # Calculate support\n",
        "        giv_s_item_sup_bmp_inner = [np.bitwise_and(a, b) for a, b in zip(givitem_sup_bmp_s[zk], sitem_sup_bmp[zk])]\n",
        "        giv_s_item_sup_bmp.append(giv_s_item_sup_bmp_inner)\n",
        "        sum_sup = __builtins__.sum(giv_s_item_sup_bmp_inner)\n",
        "        giv_s_sup += sum_sup\n",
        "        if(sum_sup!=0):\n",
        "          giv_s_seq_sum += (zk+1)\n",
        "\n",
        "        # Calculate LRU\n",
        "        giv_s_item_lru_bmp_inner = [x * y for x, y in zip(givitem_rbu_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_lru_bmp.append(giv_s_item_lru_bmp_inner)\n",
        "        giv_s_lru += __builtins__.sum(giv_s_item_lru_bmp_inner)\n",
        "\n",
        "        # Calculate Utility\n",
        "        giv_s_util_sum_bmp_inner = [__builtins__.sum(x) for x in zip(givitem_util_bmp_s[zk], sitem_util_bmp[zk])]\n",
        "        giv_s_item_util_bmp_inner = [x * y for x, y in zip(giv_s_util_sum_bmp_inner, giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_util_bmp.append(giv_s_item_util_bmp_inner)\n",
        "        giv_s_util += __builtins__.sum(giv_s_item_util_bmp_inner)\n",
        "\n",
        "        # Calculate remaining utility\n",
        "        giv_s_item_remutil_bmp_inner = [x * y for x, y in zip(sitem_rem_util_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_remutil_bmp.append(giv_s_item_remutil_bmp_inner)\n",
        "        giv_s_remutil += __builtins__.sum(giv_s_item_remutil_bmp_inner)\n",
        "\n",
        "        # Calculate RBU\n",
        "        giv_s_item_rbu_bmp_inner = [__builtins__.sum(x) for x in\n",
        "                                    zip(giv_s_item_util_bmp_inner, giv_s_item_remutil_bmp_inner)]\n",
        "        sum_rbu = __builtins__.sum(giv_s_item_rbu_bmp_inner)\n",
        "        giv_s_rbu += sum_rbu\n",
        "        giv_s_item_rbu_bmp_inner = [sum_rbu] * len(giv_s_item_rbu_bmp_inner)\n",
        "        giv_s_item_rbu_bmp.append(giv_s_item_rbu_bmp_inner)\n",
        "\n",
        "        # Calculate SWU\n",
        "        giv_s_item_swu_bmp_inner = [x * y for x, y in zip(sitem_swu_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_swu_bmp.append(giv_s_item_swu_bmp_inner)\n",
        "        giv_s_swu += __builtins__.sum(giv_s_item_swu_bmp_inner)\n",
        "\n",
        "        # Calculate remaining item count\n",
        "        giv_s_item_pos_rem_itc_bmp_inner = [x * y for x, y in\n",
        "                                            zip(sitem_pos_rem_itc_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_pos_rem_itc_bmp.append(giv_s_item_pos_rem_itc_bmp_inner)\n",
        "        giv_s_pos_rem_itc += __builtins__.sum(giv_s_item_pos_rem_itc_bmp_inner)\n",
        "\n",
        "        # Calculate S-bitmap and S-Util-bitmap\n",
        "        first_nonzero_index = next((i for i, x in enumerate(giv_s_item_sup_bmp_inner) if x != 0), None)\n",
        "        if first_nonzero_index is not None:\n",
        "              _util_ = giv_s_item_util_bmp_inner[first_nonzero_index]\n",
        "              giv_s_item_sup_bmp_s.append([0]*(first_nonzero_index+1) + [1]*(len(giv_s_item_sup_bmp_inner)-first_nonzero_index-1))\n",
        "              giv_s_item_util_bmp_s.append([0]*(first_nonzero_index+1) + [_util_]*(len(giv_s_item_sup_bmp_inner)-first_nonzero_index-1))\n",
        "        else:\n",
        "              giv_s_item_sup_bmp_s.append([0]*len(giv_s_item_sup_bmp_inner))\n",
        "              giv_s_item_util_bmp_s.append([0]*len(giv_s_item_sup_bmp_inner))\n",
        "\n",
        "\n",
        "      # Check which items are worthy of extension\n",
        "      if(giv_s_sup>=ms and giv_s_lru>=mu):\n",
        "        s_i_temp.append(s_i)\n",
        "\n",
        "        itl_org_cps = copy.deepcopy(item_l)\n",
        "        itl_org_cps.append(s_i)\n",
        "\n",
        "        # Insert the S-extended new pattern info into global dictionary\n",
        "        item_info_dbv[tuple(itl_org_cps)] = {\n",
        "\n",
        "                         'sup_bmp_sep': giv_s_item_sup_bmp, #--->>> ok\n",
        "                         'util_bmp_sep': giv_s_item_util_bmp, #--->>> ok\n",
        "                         'rem_util_bmp_sep': giv_s_item_remutil_bmp, #--->>> ok\n",
        "                         'rbu_bmp_sep': giv_s_item_rbu_bmp, #--->>> ok\n",
        "                         'lru_bmp_sep': giv_s_item_lru_bmp, #--->>> ok\n",
        "                         'swu_bmp_sep': giv_s_item_swu_bmp, #--->>> ok\n",
        "                         'pos_bmp_sep': giv_s_item_pos_rem_itc_bmp, #--->>> ok\n",
        "                         'sup_bmp_s_sep': giv_s_item_sup_bmp_s, #--->>> ok\n",
        "                         'util_bmp_s_sep': giv_s_item_util_bmp_s, #--->>> ok\n",
        "\n",
        "                         'support': giv_s_sup,\n",
        "                         'utility': giv_s_util,\n",
        "                         'seq_sum': giv_s_seq_sum,\n",
        "                         'rem_utils': giv_s_remutil,\n",
        "                         'swu': giv_s_swu,\n",
        "                         'rbu': giv_s_rbu,\n",
        "                         'lru': giv_s_lru,\n",
        "                         'rem_item_count': giv_s_pos_rem_itc\n",
        "\n",
        "                         }\n",
        "\n",
        "    # Check whether the S-extended new pattern has potential to generate high-utility super-patterns\n",
        "    for sindx_tmp in range(len(s_i_temp)):\n",
        "      s_i_main = s_i_temp[sindx_tmp]\n",
        "      gs_item = item_l + [s_i_main]\n",
        "      gs_rbu = item_info_dbv[tuple(gs_item)]['rbu']\n",
        "      if(gs_rbu>=mu):\n",
        "        item_l_child.append(gs_item) # if RBU > MinUtil, then extendable\n",
        "      else:\n",
        "        del item_info_dbv[tuple(gs_item)] # otherwise not\n",
        "\n",
        "    chus_cand_set += item_l_child # Get all the child patterns of item_l\n",
        "    chus_cand_set += exp_siblings(item_l_child, s_i_temp, s_i_temp, ms, mu) # Explore their siblings\n",
        "\n",
        "    # I - Extension\n",
        "    for iindx in range(len(i_items)): # for each s-items\n",
        "      i_i = i_items[iindx]\n",
        "\n",
        "      if((i_i not in item_neighbour[item_l[-1]]) or (i_i in item_l)): # if i-item is not neighbour to item_l\n",
        "        continue # then no need to extend with it\n",
        "\n",
        "      # Otherwise, explore with it\n",
        "      # Get corresponding parameters\n",
        "      givitem_sup_bmp_s = item_info_dbv[tuple(item_l)]['sup_bmp_sep'] # Modified\n",
        "      givitem_rbu_bmp = item_info_dbv[tuple(item_l)]['rbu_bmp_sep']\n",
        "      sitem_sup_bmp = item_info_dbv[tuple([i_i])]['sup_bmp_sep']\n",
        "\n",
        "      givitem_util_bmp_s = item_info_dbv[tuple(item_l)]['util_bmp_sep'] # Modified\n",
        "      sitem_util_bmp = item_info_dbv[tuple([i_i])]['util_bmp_sep']\n",
        "      sitem_rem_util_bmp = item_info_dbv[tuple([i_i])]['rem_util_bmp_sep']\n",
        "\n",
        "      sitem_swu_bmp = item_info_dbv[tuple([i_i])]['swu_bmp_sep']\n",
        "      sitem_pos_rem_itc_bmp = item_info_dbv[tuple([i_i])]['pos_bmp_sep']\n",
        "\n",
        "      giv_s_item_sup_bmp = []\n",
        "      giv_s_item_lru_bmp = []\n",
        "      giv_s_item_util_bmp = []\n",
        "      giv_s_item_remutil_bmp = []\n",
        "      giv_s_item_rbu_bmp = []\n",
        "      giv_s_item_swu_bmp = []\n",
        "      giv_s_item_pos_rem_itc_bmp = []\n",
        "\n",
        "      giv_s_sup = 0\n",
        "      giv_s_lru = 0\n",
        "      giv_s_util = 0\n",
        "      giv_s_seq_sum = 0\n",
        "      giv_s_remutil = 0\n",
        "      giv_s_swu = 0\n",
        "      giv_s_rbu = 0\n",
        "      giv_s_pos_rem_itc = 0\n",
        "\n",
        "      giv_s_item_sup_bmp_s = []\n",
        "      giv_s_item_util_bmp_s = []\n",
        "\n",
        "      # Calculate corresponding support, utility, remaining utility, SWU, RBU, LRU etc.\n",
        "      for zk in range(len(givitem_sup_bmp_s)):\n",
        "\n",
        "        # Calculate support\n",
        "        giv_s_item_sup_bmp_inner = [np.bitwise_and(a, b) for a, b in zip(givitem_sup_bmp_s[zk], sitem_sup_bmp[zk])]\n",
        "        giv_s_item_sup_bmp.append(giv_s_item_sup_bmp_inner)\n",
        "        sum_sup = __builtins__.sum(giv_s_item_sup_bmp_inner)\n",
        "        giv_s_sup += sum_sup\n",
        "        if(sum_sup!=0):\n",
        "          giv_s_seq_sum += (zk+1)\n",
        "\n",
        "        # Calculate LRU\n",
        "        giv_s_item_lru_bmp_inner = [x * y for x, y in zip(givitem_rbu_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_lru_bmp.append(giv_s_item_lru_bmp_inner)\n",
        "        giv_s_lru += __builtins__.sum(giv_s_item_lru_bmp_inner)\n",
        "\n",
        "        # Calculate Utility\n",
        "        giv_s_util_sum_bmp_inner = [__builtins__.sum(x) for x in zip(givitem_util_bmp_s[zk], sitem_util_bmp[zk])]\n",
        "        giv_s_item_util_bmp_inner = [x * y for x, y in zip(giv_s_util_sum_bmp_inner, giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_util_bmp.append(giv_s_item_util_bmp_inner)\n",
        "        giv_s_util += __builtins__.sum(giv_s_item_util_bmp_inner)\n",
        "\n",
        "        # Calculate remaining utility\n",
        "        giv_s_item_remutil_bmp_inner = [x * y for x, y in zip(sitem_rem_util_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_remutil_bmp.append(giv_s_item_remutil_bmp_inner)\n",
        "        giv_s_remutil += __builtins__.sum(giv_s_item_remutil_bmp_inner)\n",
        "\n",
        "        # Calculate RBU\n",
        "        giv_s_item_rbu_bmp_inner = [__builtins__.sum(x) for x in\n",
        "                                    zip(giv_s_item_util_bmp_inner, giv_s_item_remutil_bmp_inner)]\n",
        "        sum_rbu = __builtins__.sum(giv_s_item_rbu_bmp_inner)\n",
        "        giv_s_rbu += sum_rbu\n",
        "        giv_s_item_rbu_bmp_inner = [sum_rbu] * len(giv_s_item_rbu_bmp_inner)\n",
        "        giv_s_item_rbu_bmp.append(giv_s_item_rbu_bmp_inner)\n",
        "\n",
        "        # Calculate SWU\n",
        "        giv_s_item_swu_bmp_inner = [x * y for x, y in zip(sitem_swu_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_swu_bmp.append(giv_s_item_swu_bmp_inner)\n",
        "        giv_s_swu += __builtins__.sum(giv_s_item_swu_bmp_inner)\n",
        "\n",
        "        # Calculate remaining item count\n",
        "        giv_s_item_pos_rem_itc_bmp_inner = [x * y for x, y in\n",
        "                                            zip(sitem_pos_rem_itc_bmp[zk], giv_s_item_sup_bmp_inner)]\n",
        "        giv_s_item_pos_rem_itc_bmp.append(giv_s_item_pos_rem_itc_bmp_inner)\n",
        "        giv_s_pos_rem_itc += __builtins__.sum(giv_s_item_pos_rem_itc_bmp_inner)\n",
        "\n",
        "\n",
        "      # Check which items are worthy of extension\n",
        "      if(giv_s_sup>=ms and giv_s_lru>=mu):\n",
        "        i_i_temp.append(i_i)\n",
        "\n",
        "        itl_org_cps = copy.deepcopy(item_l)\n",
        "        itl_org_cps.append(i_i)\n",
        "\n",
        "        # Insert the S-extended new pattern info into global dictionary\n",
        "        item_info_dbv[tuple(itl_org_cps)] = {\n",
        "\n",
        "                         'sup_bmp_sep': giv_s_item_sup_bmp, #--->>> ok\n",
        "                         'util_bmp_sep': giv_s_item_util_bmp, #--->>> ok\n",
        "                         'rem_util_bmp_sep': giv_s_item_remutil_bmp, #--->>> ok\n",
        "                         'rbu_bmp_sep': giv_s_item_rbu_bmp, #--->>> ok\n",
        "                         'lru_bmp_sep': giv_s_item_lru_bmp, #--->>> ok\n",
        "                         'swu_bmp_sep': giv_s_item_swu_bmp, #--->>> ok\n",
        "                         'pos_bmp_sep': giv_s_item_pos_rem_itc_bmp, #--->>> ok\n",
        "                         'sup_bmp_s_sep': giv_s_item_sup_bmp_s, #--->>> ok\n",
        "                         'util_bmp_s_sep': giv_s_item_util_bmp_s, #--->>> ok\n",
        "\n",
        "                         'support': giv_s_sup,\n",
        "                         'utility': giv_s_util,\n",
        "                         'seq_sum': giv_s_seq_sum,\n",
        "                         'rem_utils': giv_s_remutil,\n",
        "                         'swu': giv_s_swu,\n",
        "                         'rbu': giv_s_rbu,\n",
        "                         'lru': giv_s_lru,\n",
        "                         'rem_item_count': giv_s_pos_rem_itc\n",
        "\n",
        "                         }\n",
        "\n",
        "    # Check whether the I-extended new pattern has potential to generate high-utility super-patterns\n",
        "    for sindx_tmp in range(len(s_i_temp)):\n",
        "      i_i_main = i_i_temp[sindx_tmp]\n",
        "      gs_item = item_l + [i_i_main]\n",
        "      gs_rbu = item_info_dbv[tuple(gs_item)]['rbu']\n",
        "      if(gs_rbu>=mu):\n",
        "        item_l_child.append(gs_item) # if RBU > MinUtil, then extendable\n",
        "      else:\n",
        "        del item_info_dbv[tuple(gs_item)] # otherwise not\n",
        "\n",
        "\n",
        "    chus_cand_set += item_l_child # Get all the child patterns of item_l\n",
        "    chus_cand_set += exp_siblings(item_l_child, s_i_temp, i_i_temp, ms, mu) # Explore their siblings\n",
        "\n",
        "    return chus_cand_set # Return the candidate CHUS set\n",
        "\n",
        "  else: # if pattern item_l avoidable\n",
        "    return [] # then return empty list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks if a sub-list is contained within a super-list"
      ],
      "metadata": {
        "id": "jWCfSW-HuH22"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm3QtbTnXNTX"
      },
      "outputs": [],
      "source": [
        "def is_sublist(sub_list, super_list):\n",
        "  \"\"\"Checks if a sub-list is contained within a super-list\"\"\"\n",
        "  return all(x in super_list for x in sub_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to eliminate non-closed patterns from the candidate CHUS set using 2-level hash function"
      ],
      "metadata": {
        "id": "btJU5GspugBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRYolcFhrPEg"
      },
      "outputs": [],
      "source": [
        "def elim_non_closed(cand_set, minutl):\n",
        "\n",
        "  \"\"\"\n",
        "    Eliminate non-closed patterns from the candidate CHUS set using 2-level hash function\n",
        "  \"\"\"\n",
        "  chus_set = []\n",
        "  key_found_bef = {}\n",
        "  for ik in range(len(cand_set)): # For each candidate\n",
        "    if isinstance(cand_set[ik], int):\n",
        "       its = cand_set[ik]\n",
        "       itl = []\n",
        "       itl.append(its)\n",
        "       item_l = itl\n",
        "    else:\n",
        "      item_l = cand_set[ik]\n",
        "    if(item_info_dbv[tuple(item_l)]['utility']>=minutl): # if it is a high-utility pattern\n",
        "      key = item_info_dbv[tuple(item_l)]['seq_sum'] # Get the summation of sequence numbers where it occured\n",
        "      # Create a hash-entry with the summation of sequence numbers as key and item_l as value\n",
        "      if(key not in key_found_bef):\n",
        "        chus_hash[key] = []\n",
        "        chus_hash[key].append(item_l)\n",
        "        key_found_bef[key] = 1\n",
        "      else:\n",
        "        chus_hash[key].append(item_l)\n",
        "        key_found_bef[key] += 1\n",
        "\n",
        "  for key, value in chus_hash.items(): # for each hash entry\n",
        "\n",
        "    val1 = copy.deepcopy(value)\n",
        "    val2 = copy.deepcopy(value)\n",
        "    for i1 in val1:\n",
        "      f=0\n",
        "      for i2 in reversed(val2):\n",
        "        # if pattern i1 and i2 has same support\n",
        "        if((i1 != i2) and (item_info_dbv[tuple(i1)]['support'] == item_info_dbv[tuple(i2)]['support'])):\n",
        "          if(is_sublist(i1,i2)): # if pattern i1 is a sublist of i2\n",
        "            f=1\n",
        "            break # i1 can't be a CHUS and so no need to check further\n",
        "          elif(is_sublist(i2,i1)): # if pattern i2 is a sublist of i1\n",
        "            indx = val2.index(i2)\n",
        "            # delete i2 as i2 can't be a CHUS\n",
        "            del val2[indx]\n",
        "            del val1[indx]\n",
        "\n",
        "      if(f==0):\n",
        "        chus_set.append(i1) # if i1 is a CHUS, then append it to chus_set\n",
        "\n",
        "  return chus_set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate the pattern distance between pr and p"
      ],
      "metadata": {
        "id": "tlloL4sZxAco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pattern_dist(pr,p):\n",
        "\n",
        "  \"\"\"\n",
        "    Calculate the pattern distance between pr and p\n",
        "  \"\"\"\n",
        "\n",
        "  pr_sup = item_info_dbv[tuple(pr)]['support']\n",
        "  pr_util = item_info_dbv[tuple(pr)]['utility']\n",
        "\n",
        "  p_sup = item_info_dbv[tuple(p)]['support']\n",
        "  p_util = item_info_dbv[tuple(p)]['utility']\n",
        "\n",
        "  common_elements = list(set(pr) & set(p))\n",
        "\n",
        "  sup_dist = 1 - (pr_sup/p_sup) # Calculate Support distance\n",
        "  patt_dist = 1 - (len(common_elements)/len(p)) # Calculate Pattern similarity\n",
        "  util_dist = (abs(pr_util-p_util))/(max(pr_util,p_util)) # Calculate Utility distance\n",
        "\n",
        "  ov_dist = (sup_dist * 2) + (patt_dist * 6) + (util_dist * 2) # Calculate Overall Pattern Distance\n",
        "\n",
        "  return ov_dist"
      ],
      "metadata": {
        "id": "86tGY93MzvMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to find neighbours within Redundancy Limit Threshold (Eps)\n"
      ],
      "metadata": {
        "id": "3nF00RlbxyH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findNeighbours(D,P,eps):\n",
        "\n",
        "    \"\"\"\n",
        "      Function to find neighbours within Redundancy Limit Threshold (Eps)\n",
        "    \"\"\"\n",
        "\n",
        "    neighbours=[] ##empty list of indices\n",
        "    for pnt in range(0,len(D)):\n",
        "        if pnt!=P:\n",
        "            if ((calculate_pattern_dist(D[P],D[pnt])) < eps): # Pattern distance is less than eps\n",
        "                neighbours.append(pnt) # add neighbour\n",
        "\n",
        "    return neighbours"
      ],
      "metadata": {
        "id": "eMQQCShjz3-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FshJ21ZRMuiU"
      },
      "source": [
        "# Proposed RACLOHusp algorithm to perform density based pattern clustering\n",
        "\n",
        "## Algorithm Here\n",
        "\n",
        "### Input:\n",
        " - chus : a set containing CHUS patterns\n",
        " - eps : the radius parameter, **(Redundancy Limit Threshold)**\n",
        " - MinPts : the neighborhood density threshold **(Minimum number of CHUS patterns which will be represented by the Core pattern of the cluster)**\n",
        "\n",
        "### Output: A set of density based clusters.\n",
        "\n",
        "### Method\n",
        "\n",
        "mark all objects as unvisited;\n",
        "- do\n",
        "    - randomly select an unvisited object p;\n",
        "    - mark p as visited;\n",
        "    - if the eps-neighborhood of p has at least MinPts objects\n",
        "        - create a new cluster C, and add p to C;\n",
        "        - let N be the set of objects in the eps-neighborhood of p;\n",
        "        - for each point p' in N\n",
        "            - if p' is unvisited\n",
        "                - mark p' as visited;\n",
        "                - if the eps-neighborhood of p' has at least MinPts points\n",
        "                - add those points to N;\n",
        "            - if p' is not yet a member of any cluster, add p' to C;\n",
        "        - end for\n",
        "        - output C;\n",
        "    - else mark p as noise;\n",
        "- until no object is unvisited;\n",
        "\n",
        "---------------------------------------------------------------------------------------------------\n",
        "I am following the DBSCAN format of Data Mining Concepts and Techniques by Jiawei Han, Micheline Kamber and Jian Pei\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering(_chus_,eps,MinPts):\n",
        "\n",
        "  \"\"\"\n",
        "    Proposed RACLOHusp algorithm to perform density based pattern clustering\n",
        "  \"\"\"\n",
        "  cluster_id = 0\n",
        "  labels = [0]*len(_chus_)\n",
        "  rep_patterns = []\n",
        "  _chus_.sort(key=len)\n",
        "  for i in reversed(range(len(_chus_))):\n",
        "\n",
        "    if i not in visited:\n",
        "      visited.append(i) ## mark as visited\n",
        "      rep_patterns.append(_chus_[i])\n",
        "      cluster_info_per_reps[tuple(_chus_[i])] = []\n",
        "      neighbourPts=findNeighbours(_chus_,i,eps)\n",
        "      if len(neighbourPts)<MinPts:\n",
        "          labels[i]=-1 ## label point as noise/outlier\n",
        "          cluster_info[tuple(_chus_[i])]=-1\n",
        "      else:\n",
        "          labels[i]=cluster_id\n",
        "          cluster_info[tuple(_chus_[i])]=cluster_id\n",
        "          ## expand cluster\n",
        "          for p in neighbourPts:\n",
        "              if p not in visited:\n",
        "                 visited.append(p) ## mark as visited\n",
        "                 pneighbourPts=findNeighbours(_chus_,p,eps)\n",
        "\n",
        "                 ## if p in branch point add it's neightbours in neighbourPts\n",
        "                 if len(pneighbourPts)>=MinPts:\n",
        "                      for pnt in pneighbourPts:\n",
        "                          if pnt not in neighbourPts:\n",
        "                              neighbourPts.append(pnt) ## adding patterns to neighbourhood\n",
        "              ## if p not in any cluster\n",
        "              if labels[p]==0:\n",
        "                  labels[p]=cluster_id\n",
        "                  cluster_info[tuple(_chus_[p])]=cluster_id\n",
        "                  cluster_info_per_reps[tuple(_chus_[i])].append(_chus_[p])\n",
        "\n",
        "          cluster_id+=1 ## create a new cluster\n",
        "\n",
        "  return labels,rep_patterns"
      ],
      "metadata": {
        "id": "Rx4OPb7c0EMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function -\n",
        "*    It performs dataset scan\n",
        "*    It removes low utility and low frequent patterns\n",
        "*    It creates dynamic vectors like support, utility, remaining utility, SWU, RBU, LRU etc. of items\n",
        "*    It calls the CLOHusp and RACLOHusp algorithms to generate CHUS patterns and perform density based pattern clustering to generate RACHUS patterns."
      ],
      "metadata": {
        "id": "3GVMb7nA1N9U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbZR2TZGttIu"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "  \"\"\"\n",
        "    Main function -\n",
        "      - It performs dataset scan\n",
        "      - It removes low utility and low frequent patterns\n",
        "      - It creates dynamic vectors like support, utility, remaining utility, SWU, RBU, LRU etc. of items\n",
        "      - It calls the CLOHusp and RACLOHusp algorithms to generate CHUS patterns and perform density based pattern clustering to generate RACHUS patterns.\n",
        "  \"\"\"\n",
        "\n",
        "  filename = '/content/drive/MyDrive/MSThesis/CHUS-4/Dataset/Mushroom.txt' # Read dataset from drive like, Mushroom\n",
        "\n",
        "  file = open(filename, 'r')\n",
        "\n",
        "  unique_item_set = set()\n",
        "\n",
        "\n",
        "  # Initialize some free lists\n",
        "  item_list_rep_sup = [0] * 1570\n",
        "  item_list_rep_util = [0] * 1570\n",
        "  item_list_rep_remutil = [0] * 1570\n",
        "  item_list_rep_remicount = [0] * 1570\n",
        "  item_list_rep_total_swu = [0] * 1570\n",
        "  item_list_rep_total_rbu = [0] * 1570\n",
        "  item_seq_sum = [0] * 1570\n",
        "\n",
        "  sp=0\n",
        "  ut=0\n",
        "  rm=0\n",
        "  seq_sum = 0\n",
        "  tran_util=0\n",
        "  item_cswu=0\n",
        "  item_crbu=0\n",
        "  rm_it_cnt=0\n",
        "  i=0\n",
        "  total_util = 0\n",
        "\n",
        "  for line in file: # Process each sequences\n",
        "\n",
        "    row = [int(x) for x in line.replace(\":\", \" \").split()]\n",
        "\n",
        "    m = math.floor(len(row)/2)\n",
        "    seq_util_val.append(row[m]) # Get Sequence utility\n",
        "    tran_util = row[m]\n",
        "    total_util += row[m]\n",
        "    ul = row[m+1:]\n",
        "    sl = row[:m]\n",
        "    temp_set = set(sl)\n",
        "    unique_item_set.update(temp_set)\n",
        "\n",
        "    temp_bitmap_inner = [0] * len(sl)\n",
        "    temp_bitmap.append(temp_bitmap_inner)\n",
        "\n",
        "    for z in range(len(sl)):\n",
        "\n",
        "        # Calculate Support\n",
        "        item_list_rep_sup[sl[z]-1] += 1\n",
        "        sp = item_list_rep_sup[sl[z]-1]\n",
        "\n",
        "        # Calculate Utility\n",
        "        item_list_rep_util[sl[z]-1] += ul[z]\n",
        "        ut = item_list_rep_util[sl[z]-1]\n",
        "\n",
        "        # Calculate Summation of sequences\n",
        "        item_seq_sum[sl[z]-1] += (i+1)\n",
        "        seq_sum = item_seq_sum[sl[z]-1]\n",
        "\n",
        "        # Calculate Remaining utility\n",
        "        item_list_rep_remutil[sl[z]-1] += __builtins__.sum(ul[(z+1):])\n",
        "        rm = item_list_rep_remutil[sl[z]-1]\n",
        "\n",
        "        # Calculate SWU\n",
        "        item_list_rep_total_swu[sl[z]-1] += tran_util\n",
        "        item_cswu = item_list_rep_total_swu[sl[z]-1]\n",
        "\n",
        "        # Calculate RBU\n",
        "        item_list_rep_total_rbu[sl[z]-1] += __builtins__.sum(ul[z:])\n",
        "        item_crbu = item_list_rep_total_rbu[sl[z]-1]\n",
        "\n",
        "        # Calculate Remaining Item Count\n",
        "        item_list_rep_remicount[sl[z]-1] += len(sl)-(z+1)\n",
        "        rm_it_cnt = item_list_rep_remicount[sl[z]-1]\n",
        "\n",
        "        if(item_seen_bef[sl[z]]==0):\n",
        "\n",
        "          item_neighbour[sl[z]] = set()\n",
        "          item_neighbour[sl[z]].update(set(sl[z+1:])) # add adjacent items\n",
        "\n",
        "          seq_no[sl[z]] = []\n",
        "          seq_no[sl[z]].append(i)\n",
        "\n",
        "          seq_pos_no[sl[z]] = []\n",
        "          seq_pos_no[sl[z]].append(z)\n",
        "\n",
        "          seq_pos_no_util[sl[z]] = []\n",
        "          seq_pos_no_util[sl[z]].append(ul[z])\n",
        "\n",
        "          seq_pos_no_rem_util[sl[z]] = []\n",
        "          seq_pos_no_rem_util[sl[z]].append((__builtins__.sum(ul[(z+1):])))\n",
        "\n",
        "          seq_pos_no_rbu[sl[z]] = []\n",
        "          seq_pos_no_rbu[sl[z]].append((__builtins__.sum(ul[z:])))\n",
        "\n",
        "          seq_pos_no_lru[sl[z]] = []\n",
        "          seq_pos_no_lru[sl[z]].append(tran_util)\n",
        "\n",
        "          seq_pos_no_rem_item_count[sl[z]] = []\n",
        "          seq_pos_no_rem_item_count[sl[z]].append(len(sl)-(z+1))\n",
        "\n",
        "          item_seen_bef[sl[z]] = 1\n",
        "\n",
        "        else:\n",
        "          item_neighbour[sl[z]].update(set(sl[z+1:]))\n",
        "          seq_no[sl[z]].append(i)\n",
        "          seq_pos_no[sl[z]].append(z)\n",
        "          seq_pos_no_util[sl[z]].append(ul[z])\n",
        "          seq_pos_no_rem_util[sl[z]].append(__builtins__.sum(ul[(z+1):]))\n",
        "          seq_pos_no_rbu[sl[z]].append(__builtins__.sum(ul[z:]))\n",
        "          seq_pos_no_lru[sl[z]].append(tran_util)\n",
        "          seq_pos_no_rem_item_count[sl[z]].append(len(sl)-(z+1))\n",
        "\n",
        "        item_info[sl[z]] = {'support': sp,\n",
        "                            'utility': ut,\n",
        "                            'seq_sum': seq_sum,\n",
        "                            'rem_util': rm,\n",
        "                            'swu': item_cswu,\n",
        "                            'rbu': item_crbu,\n",
        "                            'lru': item_cswu,\n",
        "                            'rem_ic': rm_it_cnt}\n",
        "\n",
        "    util_list.append(ul)\n",
        "    seq_list.append(sl)\n",
        "    i+=1\n",
        "  file.close()\n",
        "  unique_items = list(unique_item_set)\n",
        "\n",
        "\n",
        "  mu_perc = 0.06 # Min Utility in percentage\n",
        "  minsup = 4874 # Min Support\n",
        "  minutil = int(total_util*mu_perc)\n",
        "\n",
        "  print(\"-----------General Description--------------\")\n",
        "  print(f\"Dataset: Mushroom\")\n",
        "  print(f\"Total no of transactions: {i}\")\n",
        "  print(f\"Total no of items: {len(unique_items)}\")\n",
        "  print(f\"Total Util: {total_util}\")\n",
        "  print(f\"ms: {minsup}\")\n",
        "  print(f\"mu: {minutil}\")\n",
        "  print(f\"mu-perc: {mu_perc}\")\n",
        "\n",
        "\n",
        "  rem_items = []\n",
        "\n",
        "  # Identify the frequent-1 items having high-utility\n",
        "  for key, value in item_info.items():\n",
        "    if(value['support']<minsup or value['swu']<minutil):\n",
        "      keys_to_delete.append(key)\n",
        "    else:\n",
        "      rem_items.append(key)\n",
        "\n",
        "  for key in keys_to_delete:\n",
        "    del item_info[key]\n",
        "\n",
        "  print(f\"Total no of remaining items: {len(rem_items)}\")\n",
        "  print(\"--------------------------------------------\")\n",
        "  print()\n",
        "\n",
        "\n",
        "  # Create bitmaps of support, utility, remaining utility, SWU, RBU, LRU etc. of items\n",
        "  for item_j in range (len(rem_items)):\n",
        "\n",
        "    _item_ = rem_items[item_j]\n",
        "\n",
        "    ib = copy.deepcopy(temp_bitmap)\n",
        "    ibs = copy.deepcopy(temp_bitmap)\n",
        "\n",
        "    iub = copy.deepcopy(temp_bitmap)\n",
        "    iubs = copy.deepcopy(temp_bitmap)\n",
        "\n",
        "    irub = copy.deepcopy(temp_bitmap)\n",
        "    ipb = copy.deepcopy(temp_bitmap)\n",
        "\n",
        "    i_rbu = copy.deepcopy(temp_bitmap)\n",
        "    i_lru = copy.deepcopy(temp_bitmap)\n",
        "\n",
        "    for iter in range(len(seq_no[_item_])):\n",
        "\n",
        "      seq_no_i = seq_no[_item_][iter]\n",
        "      seq_pos_no_i = seq_pos_no[_item_][iter]\n",
        "      seq_pos_no_util_i = seq_pos_no_util[_item_][iter]\n",
        "      seq_pos_no_rem_util_i = seq_pos_no_rem_util[_item_][iter]\n",
        "      seq_pos_no_rbu_i = seq_pos_no_rbu[_item_][iter]\n",
        "      seq_pos_no_lru_i = seq_pos_no_lru[_item_][iter]\n",
        "      seq_pos_no_rem_item_count_i = seq_pos_no_rem_item_count[_item_][iter]\n",
        "\n",
        "      seq_pos_no_next = seq_pos_no_i + 1\n",
        "\n",
        "      ib[seq_no_i][seq_pos_no_i] = 1\n",
        "      tmp_list = copy.deepcopy(ibs[seq_no_i])\n",
        "      tmp_list_a = tmp_list[:seq_pos_no_next]\n",
        "      tmp_list_b = tmp_list[seq_pos_no_next:]\n",
        "      tmp_list_b = [1]*len(tmp_list_b)\n",
        "      ibs[seq_no_i] = tmp_list_a + tmp_list_b\n",
        "\n",
        "      iub[seq_no_i][seq_pos_no_i] = seq_pos_no_util_i\n",
        "      tmp_list = copy.deepcopy(iubs[seq_no_i])\n",
        "      tmp_list_a = tmp_list[:seq_pos_no_next]\n",
        "      tmp_list_b = tmp_list[seq_pos_no_next:]\n",
        "      tmp_list_b = [seq_pos_no_util_i]*len(tmp_list_b)\n",
        "      iubs[seq_no_i] = tmp_list_a + tmp_list_b\n",
        "\n",
        "      irub[seq_no_i][seq_pos_no_i] = seq_pos_no_rem_util_i\n",
        "      ipb[seq_no_i][seq_pos_no_i] = seq_pos_no_rem_item_count_i\n",
        "\n",
        "      len_i_rbu = len(i_rbu[seq_no_i])\n",
        "      tmp_i_rbu = [seq_pos_no_rbu_i]*len_i_rbu\n",
        "      i_rbu[seq_no_i] = tmp_i_rbu\n",
        "\n",
        "\n",
        "      i_lru[seq_no_i][seq_pos_no_i] = seq_pos_no_lru_i\n",
        "\n",
        "    item_info_dbv[tuple([_item_])] = {\n",
        "\n",
        "                         'sup_bmp_sep': ib, #--->>> ok\n",
        "                         'util_bmp_sep': iub, #--->>> ok\n",
        "                         'rem_util_bmp_sep': irub, #--->>> ok\n",
        "                         'rbu_bmp_sep': i_rbu, #--->>> ok\n",
        "                         'lru_bmp_sep': i_lru, #--->>> ok\n",
        "                         'swu_bmp_sep': i_lru, #--->>> ok\n",
        "                         'pos_bmp_sep': ipb, #--->>> ok\n",
        "                         'sup_bmp_s_sep': ibs, #--->>> ok\n",
        "                         'util_bmp_s_sep': iubs, #--->>> ok\n",
        "\n",
        "                         'support': item_info[_item_]['support'],\n",
        "                         'utility': item_info[_item_]['utility'],\n",
        "                         'seq_sum': item_info[_item_]['seq_sum'],\n",
        "                         'rem_utils': item_info[_item_]['rem_util'],\n",
        "                         'swu': item_info[_item_]['swu'],\n",
        "                         'rbu': item_info[_item_]['rbu'],\n",
        "                         'lru': item_info[_item_]['lru'],\n",
        "                         'rem_item_count': item_info[_item_]['rem_ic']\n",
        "\n",
        "                         }\n",
        "\n",
        "  chus_cand = []\n",
        "\n",
        "  chus_cand += rem_items\n",
        "\n",
        "  for item_i in range(len(rem_items)): # For each high utility items\n",
        "\n",
        "    # Call the CLOHusp algorithm to generate candidate CHUS pattrns\n",
        "    chus_cand += dfs_pattern_ext([rem_items[item_i]],rem_items,rem_items[(item_i+1):],minsup,minutil)\n",
        "\n",
        "  # Create a Data Frame for Cand-CHUS\n",
        "  sc=[]\n",
        "  uc=[]\n",
        "  print()\n",
        "  print(f\"Total no of Cand. CHUS: {len(chus_cand)}\")\n",
        "  for i in range(len(chus_cand)):\n",
        "    if(isinstance(chus_cand[i], int)):\n",
        "      ck = [chus_cand[i]]\n",
        "    else:\n",
        "      ck = chus_cand[i]\n",
        "    sc += [item_info_dbv[tuple(ck)]['support']]\n",
        "    uc += [item_info_dbv[tuple(ck)]['utility']]\n",
        "  df_chusc = pd.DataFrame({'CHUS Candidate - Patterns': chus_cand, 'Support': sc, 'Utility': uc})\n",
        "\n",
        "  chus_set = elim_non_closed(chus_cand,minutil) # Eliminate the non-closed candidates to generate the CHUS patterns\n",
        "\n",
        "  print(f\"Total no of CHUS: {len(chus_set)}\")\n",
        "  print()\n",
        "\n",
        "  sups = [item_info_dbv[tuple(key)]['support'] for key in chus_set if tuple(key) in item_info_dbv]\n",
        "  uts  = [item_info_dbv[tuple(key)]['utility'] for key in chus_set if tuple(key) in item_info_dbv]\n",
        "\n",
        "  # Call the RACLOHusp algorithm to generate RACHUS pattrns by density based pattern clustering\n",
        "  # Here, eps = 0.73 and MinPatts = 2\n",
        "  labs,reps = clustering(chus_set,0.73,2)\n",
        "\n",
        "  # Create a Data Frame for RACHUS\n",
        "  sups_r = [item_info_dbv[tuple(key)]['support'] for key in reps if tuple(key) in item_info_dbv]\n",
        "  uts_r  = [item_info_dbv[tuple(key)]['utility'] for key in reps if tuple(key) in item_info_dbv]\n",
        "  lbls_r = [cluster_info[tuple(key)] for key in reps if tuple(key) in item_info_dbv]\n",
        "  df_chus_r = pd.DataFrame({'CHUS - Patterns': reps, 'Support': sups_r, 'Utility': uts_r, 'Cluster ID': lbls_r})\n",
        "\n",
        "  # Create a Data Frame for CHUS\n",
        "  lbls = [cluster_info[tuple(key)] for key in chus_set if tuple(key) in item_info_dbv]\n",
        "  df_chus = pd.DataFrame({'CHUS - Patterns': chus_set, 'Support': sups, 'Utility': uts, 'Cluster ID': lbls})\n",
        "\n",
        "  print(f\"Total no of Rep-CHUS: {len(reps)}\")\n",
        "  print()\n",
        "\n",
        "  return df_chus, chus_set, df_chus_r, reps, labs # Return corresponding outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARRB569qWa0n"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df, _chus_, dfr, rps, lbs = main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}